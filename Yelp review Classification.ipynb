{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arasu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "data_file = open(\"yelp_academic_dataset_review.json\",'r', encoding='utf-8')\n",
    "data = []\n",
    "p=0\n",
    "for line in data_file:\n",
    "    if(p<100000):\n",
    "        p+=1\n",
    "        continue\n",
    "    if(p>200000):\n",
    "        break\n",
    "    data.append(json.loads(line))\n",
    "    p+=1\n",
    "review_df = pd.DataFrame(data)\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_id                                 NFckLwN9S4UgFlfo0bPRxA\n",
       "user_id                                   HwcjEqA1V3bKtb07JKGIKQ\n",
       "business_id                               nqKL5PbJbwwoCK_Xon31kA\n",
       "stars                                                        4.0\n",
       "useful                                                         0\n",
       "funny                                                          0\n",
       "cool                                                           0\n",
       "text           I went here for a friend's birthday with a lar...\n",
       "date                                         2007-06-15 16:13:15\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.iloc[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipos=[(i,1) for i in review_df[(review_df['stars']==4 )|(review_df['stars']==5)]['text']]\n",
    "linos=[(i,0) for i in review_df[(review_df['stars']==1 )|(review_df['stars']==2)]['text']]\n",
    "li0=[(i,0.5) for i in review_df[(review_df['stars']==3 )]['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87849"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr=lipos+linos\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(arr)\n",
    "len(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i'm\",\n",
       " 'sure',\n",
       " 'fuss',\n",
       " '4',\n",
       " 'star',\n",
       " 'basic',\n",
       " 'hotel',\n",
       " 'restaur',\n",
       " 'overpr',\n",
       " 'regular',\n",
       " 'food',\n",
       " 'get',\n",
       " 'street',\n",
       " 'restaur',\n",
       " 'burlington',\n",
       " 'mall',\n",
       " 'decor',\n",
       " 'nice',\n",
       " 'kobe',\n",
       " 'burger',\n",
       " 'tast',\n",
       " 'like',\n",
       " 'burger',\n",
       " 'say',\n",
       " 'onion',\n",
       " 'ring',\n",
       " 'great',\n",
       " 'drink',\n",
       " 'eh',\n",
       " 'servic',\n",
       " 'great',\n",
       " 'price',\n",
       " 'menu',\n",
       " 'weird',\n",
       " \"they'r\",\n",
       " 'wear',\n",
       " 'jean',\n",
       " 'speak',\n",
       " 'custom',\n",
       " 'expect',\n",
       " 'spoken',\n",
       " 'burger',\n",
       " 'king',\n",
       " 'probabl',\n",
       " 'come',\n",
       " 'back']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_tweet(tweet):\n",
    "    tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n",
    "\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet2 = re.sub(r'#', '', tweet2)\n",
    "\n",
    "    # instantiate tokenizer class\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                                   reduce_len=True)\n",
    "\n",
    "    # tokenize tweets\n",
    "    tweet_tokens = tokenizer.tokenize(tweet2)\n",
    "    stopwords_english = stopwords.words('english') \n",
    "    tweets_clean = []\n",
    "\n",
    "    for word in tweet_tokens: # Go through every word in your tokens list\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            tweets_clean.append(word)\n",
    "    stemmer = PorterStemmer() \n",
    "\n",
    "    # Create an empty list to store the stems\n",
    "    tweets_stem = [] \n",
    "\n",
    "    for word in tweets_clean:\n",
    "        stem_word = stemmer.stem(word)  # stemming word\n",
    "        tweets_stem.append(stem_word)  # append to the list\n",
    "\n",
    "    #print('stemmed words:')\n",
    "    #print(tweets_stem)\n",
    "    return tweets_stem\n",
    "import random\n",
    "x=random.randint(1,(len(arr)))\n",
    "process_tweet(arr[x][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split \n",
    "train=arr[:(len(arr)*80//100)]\n",
    "test=arr[(len(arr)*80//100):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "def freq(arr): # Creating frequency dictionary\n",
    "    freqs=dict()\n",
    "    k=0\n",
    "    for i in arr:\n",
    "        k+=1\n",
    "        if k%10000==0:\n",
    "            print(k//10000)\n",
    "        for j in process_tweet(i[0]):\n",
    "            if (j,i[1]) in freqs:\n",
    "                freqs[(j,i[1])]=freqs[(j,i[1])]+1\n",
    "            else:\n",
    "                freqs[(j,i[1])]=1\n",
    "    return freqs\n",
    "freqs=freq(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "tweets=[i[0] for i in train]\n",
    "y=[i[1] for i in train]\n",
    "# Exract features for training set using frequency dictionary\n",
    "\n",
    "def extract_features(tweet, freqs):\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "    for word in word_l:\n",
    "        if (word,1) in freqs:\n",
    "            x[0,1] += freqs[(word,1)]\n",
    "        if (word,0) in freqs:\n",
    "            x[0,2] += freqs[(word,0)]\n",
    "    norm = np. linalg. norm(x)\n",
    "    x=x/norm\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x\n",
    "xfeats=np.empty((1,3))\n",
    "p=0\n",
    "for i in tweets:\n",
    "    if(p%10000==0):\n",
    "        print(p//10000)\n",
    "    p+=1\n",
    "    xfeats=np.append(xfeats,extract_features(i, freqs), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=np.array(y).reshape(-1,1)\n",
    "xfeats1=np.delete(xfeats,0,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.69314718]] 0\n",
      "[[0.38701409]] 10000\n",
      "[[0.35170652]] 20000\n",
      "[[0.33888552]] 30000\n",
      "[[0.33308305]] 40000\n",
      "[[0.33012871]] 50000\n",
      "[[0.32851276]] 60000\n",
      "[[0.32758609]] 70000\n",
      "[[0.32703691]] 80000\n",
      "[[0.32670361]] 90000\n",
      "[[0.32649773]] 100000\n",
      "[[0.32636884]] 110000\n",
      "[[0.32628732]] 120000\n",
      "[[0.32623534]] 130000\n",
      "[[0.32620198]] 140000\n",
      "[[0.32618047]] 150000\n",
      "[[0.32616653]] 160000\n",
      "[[0.32615746]] 170000\n",
      "[[0.32615155]] 180000\n",
      "[[0.32614768]] 190000\n",
      "[[0.32614514]] 200000\n",
      "[[0.32614346]] 210000\n",
      "[[0.32614235]] 220000\n",
      "[[0.32614161]] 230000\n",
      "[[0.32614111]] 240000\n",
      "[[0.32614076]] 250000\n",
      "[[0.32614053]] 260000\n",
      "[[0.32614036]] 270000\n",
      "[[0.32614023]] 280000\n",
      "[[0.32614013]] 290000\n",
      "[[0.32614006]] 300000\n",
      "[[0.32613999]] 310000\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-1*z))\n",
    "def cost_function(theta,X,y):\n",
    "    z=np.dot(X,theta)\n",
    "    #print(z)\n",
    "    h=sigmoid(z)\n",
    "    #print(h)\n",
    "    return (-1/len(X))*np.dot(np.transpose(np.log(h)),y)+(-1/len(X))*np.dot(np.transpose(np.log(1-h)),(1-y))\n",
    "def gradient(theta,X,y):\n",
    "    z=np.dot(X,theta)\n",
    "    #print(z.shape)\n",
    "    h=sigmoid(z)\n",
    "    #print((h).shape)\n",
    "    #print(y.shape)\n",
    "    return (np.dot(np.transpose(X),h-y)/len(X))\n",
    "\n",
    "m=xfeats.shape[0]\n",
    "cost_the=list()\n",
    "theta=np.zeros((3, 1))\n",
    "alpha=1e-1\n",
    "num_iters=320000\n",
    "# Training Logistic Regression model\n",
    "for i in range(num_iters):\n",
    "    \n",
    "    J=cost_function(theta,xfeats1,y1)\n",
    "    \n",
    "    theta=theta-alpha*gradient(theta,xfeats1,y1)\n",
    "\n",
    "    if(i%10000==0):\n",
    "        print(J,i)\n",
    "        cost_the.append((J,theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Extracting features for test set\n",
    "tweets1=np.array([i[0] for i in test])\n",
    "ytest=np.array([i[1] for i in test])\n",
    "xfeats2=np.empty((1,3))\n",
    "p=0\n",
    "for i in tweets1:\n",
    "    if(p%10000==0):\n",
    "        print(p//10000)\n",
    "    p+=1 \n",
    "    xfeats2=np.append(xfeats2,extract_features(i, freqs), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.21248413],\n",
       "       [ 12.90096677],\n",
       "       [-28.69168818]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8578258394991463"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test=np.delete(xfeats2,0,axis=0)\n",
    "ypreds=[0 if i<0.5 else 1 for i in sigmoid(np.dot(x_test,theta))]\n",
    "sum([1 if(ytest[i]==ypreds[i]) else 0 for i in range(len(ypreds))])/len(ypreds) # Accuracy score on test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
